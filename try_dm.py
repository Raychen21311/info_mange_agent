import streamlit as st
import os
import time
import fitz
import google.generativeai as genai
import pickle
from langchain_community.vectorstores import FAISS
from langchain.docstore.document import Document
from dotenv import load_dotenv
from langchain_huggingface import HuggingFaceEmbeddings
    
from docx import Document as DocxDocument
import pandas as pd
#from pptx import Presentation
from PyPDF2 import PdfReader
import torch
import faiss

# --- Gemini è¨­å®š ---
load_dotenv()
if os.getenv('GOOGLE_API_KEY'):
    genai.configure(api_key=os.getenv('GOOGLE_API_KEY'))
gen_model = genai.GenerativeModel("gemini-2.5-flash")
device = "cuda" if torch.cuda.is_available() else "cpu"

# --- å‘é‡æ¨¡å‹ ---
embedding_model = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-mpnet-base-v2",
    model_kwargs={"device": device}  # âœ… æŒ‡å®šè¨­å‚™
)
# --- æ–‡ä»¶è®€å–èˆ‡åˆ‡æ®µè½ï¼ˆæ ¸å¿ƒçŸ¥è­˜åº«ï¼‰ ---


# --- å»ºç«‹å‘é‡è³‡æ–™åº« (FAISS) â€” AESâ€‘GCM è§£å¯† + åˆ—å‡ºæª”æ¡ˆæ¸…å–® ---
import io, zipfile, tempfile, base64, time
from pathlib import Path
from cryptography.hazmat.primitives.ciphers.aead import AESGCM
from langchain_community.vectorstores import FAISS

start = time.perf_counter()

# ç›´æ¥æŒ‡å®šåŠ å¯†æª”è·¯å¾‘ï¼ˆç›¸å°æ–¼ try_dm.pyï¼‰
enc_path = Path(__file__).resolve().parent / "file_path" / "faiss_index.enc"

def _decrypt_bytes(blob: bytes, key_b64: str) -> bytes:
    key = base64.urlsafe_b64decode(key_b64)  # 32 bytes -> AESâ€‘256â€‘GCM
    nonce, ct = blob[:12], blob[12:]
    return AESGCM(key).decrypt(nonce, ct, associated_data=None)

try:
    if enc_path.exists():
        with open(enc_path, "rb") as f:
            blob = f.read()
        key_b64 = st.secrets["FAISS_KEY_B64"]  # é‡‘é‘°è«‹æ”¾åœ¨ Secrets
        zip_bytes = _decrypt_bytes(blob, key_b64)

        # è§£å£“ ZIP åˆ°æš«å­˜è³‡æ–™å¤¾
        tmp_dir = Path(tempfile.mkdtemp(prefix="faiss_"))
        with zipfile.ZipFile(io.BytesIO(zip_bytes), "r") as zf:
            zf.extractall(tmp_dir)

        # âœ… åˆ—å‡ºè§£å£“å¾Œæª”æ¡ˆæ¸…å–®ï¼ˆç”¨ printï¼‰
        print("ğŸ“‚ è§£å£“å¾Œæª”æ¡ˆæ¸…å–®ï¼š")
        for p in tmp_dir.rglob("*"):
            print(" -", p.relative_to(tmp_dir))

        # å˜—è©¦ç›´æ¥è¼‰å…¥ï¼ˆå‡è¨­ ZIP æ ¹ç›®éŒ„å°±æœ‰ index.faiss å’Œ index.pklï¼‰
        vector_store = FAISS.load_local(
            str(tmp_dir),
            embeddings=embedding_model,
            allow_dangerous_deserialization=True
        )
        st.write(f"âœ… FAISS è¼‰å…¥å®Œæˆï¼ˆä¾†æºï¼š{enc_path.name}ï¼‰ï¼Œè€—æ™‚: {time.perf_counter() - start:.2f} ç§’")
    else:
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°åŠ å¯†æª”ï¼š{enc_path}")

except Exception as e:
    st.error(f"âŒ è¼‰å…¥ FAISS å¤±æ•—ï¼š{e}")
    raise

           






# --- RAG æª¢ç´¢ ---
def retrieve_context(question, top_k=3):
    start = time.perf_counter()
    docs = vector_store.similarity_search(question, k=top_k)
    search_time = time.perf_counter() - start
    st.write(f"âœ… æª¢ç´¢è€—æ™‚: {search_time:.2f} ç§’")
    return [d.page_content for d in docs]

# --- Gemini å›ç­”ç”Ÿæˆï¼ˆåŠ å…¥æ­·å²å°è©±ï¼‰ ---
def get_gemini_response(user_question, context_text, history):
    history_text = ""
    for role, msg in history:
        history_text += f"{role}: {msg}\n"

    prompt = f"""
ä½ æ˜¯ä¸€ä½è¡›ç”Ÿç¦åˆ©éƒ¨è³‡è¨Šè™•è™•é•·ï¼Œè² è²¬æä¾›é—œæ–¼é†«ç™‚æ©Ÿæ§‹è³‡è¨Šè¨ˆç•«ã€ç³»çµ±å»ºç½®ã€AI å°å…¥ï¼Œä»¥åŠç›¸é—œæ¡è³¼èˆ‡ç°½æ ¸äº‹é …çš„å°ˆæ¥­æŒ‡å°ã€‚  
ä½ ç†Ÿæ‚‰è³‡å®‰è¦ç¯„ã€å€‹è³‡ä¿è­·æ³•ã€é†«ç™‚è³‡è¨Šç®¡ç†æ³•è¦ï¼Œä»¥åŠæ”¿åºœæ¡è³¼æ³•ç­‰ç›¸é—œå›è¦†ã€‚  

ä½ çš„å›ç­”éœ€åš´è¬¹ã€å®¢è§€ï¼Œæ‡‰ä¾æ“šæ ¸å¿ƒçŸ¥è­˜åº«æä¾›çš„å…§å®¹ã€‚
è‹¥æœ‰ä½¿ç”¨è€…ä¸Šå‚³é™„ä»¶ï¼Œè«‹ä¸€ä½µè§£è®€ä½¿ç”¨è€…ä¸Šå‚³çš„é™„ä»¶å…§å®¹ã€‚
å°æ–¼åŒä¸€è­°é¡Œï¼Œå›ç­”å…§å®¹éœ€ä¿æŒä¸€è‡´ï¼Œé¿å…å‰å¾ŒçŸ›ç›¾ã€‚  
é‡å°ä¸€èˆ¬æ°‘çœ¾æˆ–éå°ˆæ¥­äººå£«ï¼Œè«‹ä»¥æ·ºé¡¯æ˜“æ‡‚çš„æ–¹å¼èªªæ˜å°ˆæ¥­è¡“èªå’Œæµç¨‹ã€‚  
è«‹é¿å…è‡†æ¸¬æˆ–æ¨è«–æœªæ˜è¼‰å…§å®¹ï¼Œå›ç­”æ™‚ä¿æŒæ­£å¼ã€æ¸…æ¥šã€æ˜“æ‡‚çš„èªæ°£ï¼Œç”¨å­—é£è©ä¾æ“šçŸ¥è­˜åº«é¢¨æ ¼ã€‚  
è«‹ä½¿ç”¨ **ç´”æ–‡å­—è¼¸å‡º**ï¼Œä¸è¦åŒ…å« HTML æ¨™ç±¤æˆ–æ ¼å¼åŒ–ç¬¦è™Ÿï¼Œä¾‹å¦‚ <b>ã€<i> ç­‰ã€‚
---å°è©±æ­·å²---
{history_text}

---æ ¸å¿ƒçŸ¥è­˜åº«åŠä½¿ç”¨è€…ä¸Šå‚³çš„é™„ä»¶å…§å®¹---
{context_text}

---ä½¿ç”¨è€…å•é¡Œ---
{user_question}
"""
    start = time.perf_counter()
    response = gen_model.generate_content(prompt)
    gemini_time = time.perf_counter() - start
    st.write(f"âœ… Gemini API è€—æ™‚: {gemini_time:.2f} ç§’")
    return response.text


import streamlit as st
import fitz

# --- åˆå§‹åŒ– ---
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []
if "user_file_text" not in st.session_state:
    st.session_state.user_file_text = ""

st.set_page_config(page_title="è³‡è¨Šè™• AI Agent", layout="wide")

# --- è‡ªè¨‚ CSS ---
st.markdown("""
<style>
.bottom-container {
    position: fixed;
    bottom: 0;
    left: 0;
    right: 0;
    padding: 12px;
    background-color: white;
    border-top: 1px solid #ddd;
    display: flex;
    align-items: center;
    gap: 8px;
}
.input-wrapper {
    flex: 1;
    position: relative;
}
.input-box {
    width: 100%;
    padding: 12px 40px 12px 12px;
    border: 1px solid #ccc;
    border-radius: 20px;
    font-size: 15px;
}
.send-btn {
    position: absolute;
    right: 6px;
    top: 10%;
    transform: translateY(-50%);
    background-color: black;
    color: white;
    border: none;
    border-radius: 50%;
    width: 28px;
    height: 28px;
    font-size: 14px;
    cursor: pointer;
}
.send-btn:hover {
    background-color: #333;
}
</style>
""", unsafe_allow_html=True)

st.title("è³‡è¨Šè™• AI Agent")

# --- èŠå¤©é¡¯ç¤ºå€ ---
chat_container = st.container()
for role, msg in st.session_state.chat_history:
    with chat_container:
        with st.chat_message("user" if role == "ä½¿ç”¨è€…" else "assistant"):
            st.markdown(msg)

# --- åº•éƒ¨è¼¸å…¥æ¡† ---
st.markdown('<div class="bottom-container">', unsafe_allow_html=True)

with st.form("chat_form", clear_on_submit=True):
    cols = st.columns([2, 8, 0.6])

    # ä¸Šå‚³æª”æ¡ˆ
    with cols[0]:
        uploaded_files = st.file_uploader(
            "ğŸ“„ æ–‡ä»¶",
            type=["pdf", "docx","doc","txt","xls","xlsx","ppt","pptx"],
            key="file_uploader",
            label_visibility="collapsed",
            accept_multiple_files=True
        )
        if uploaded_files:
            for uploaded_file in uploaded_files:  # âœ… è¿­ä»£æ¯å€‹æª”æ¡ˆ
                ext = uploaded_file.name.lower().split(".")[-1]
                file_text = ""

                if ext == "pdf":
                    pdf_document = fitz.open(stream=uploaded_file.read(), filetype="pdf")
                    for page in pdf_document:
                        file_text += page.get_text() + "\n\n"

                elif ext in ["docx", "doc"]:
                    doc = DocxDocument(uploaded_file)
                    file_text = "\n".join(p.text for p in doc.paragraphs if p.text.strip())

                elif ext == "txt":
                    file_text = uploaded_file.read().decode("utf-8")

                elif ext in ["xls", "xlsx"]:
                    df = pd.read_excel(uploaded_file)
                    file_text = df.to_csv(index=False)

                elif ext in ["ppt", "pptx"]:
                    prs = Presentation(uploaded_file)
                    for slide in prs.slides:
                        for shape in slide.shapes:
                            if hasattr(shape, "text"):
                                file_text += shape.text + "\n"
            
            
            
                st.session_state.user_file_text = file_text
                st.success(f"{uploaded_file.name} å·²æˆåŠŸè®€å–")
  


    # è¼¸å…¥æ¡† + é€å‡ºæŒ‰éˆ•
    with cols[1]:
        user_input = st.text_area("è«‹è¼¸å…¥æ‚¨çš„å•é¡Œâ€¦", key="chat_input", label_visibility="collapsed",height=50)
        
    with cols[2]:
        submit = st.form_submit_button("â¤", use_container_width=False)


st.markdown('</div>', unsafe_allow_html=True)

# --- è™•ç†è¨Šæ¯ ---
if submit and user_input:
    # 1ï¸âƒ£ é¡¯ç¤ºä½¿ç”¨è€…è¨Šæ¯
    with chat_container:
        with st.chat_message("user"):
            st.markdown(user_input)

    # 2ï¸âƒ£ åœ¨èŠå¤©å€å»ºç«‹ AI å›ç­”çš„å ä½å®¹å™¨
    with chat_container:
        answer_placeholder = st.empty()  # å ä½
        with answer_placeholder.container():
            # åœ¨å ä½å®¹å™¨è£¡å…ˆé¡¯ç¤º spinner
            with st.chat_message("assistant"):
                st.markdown(
            """
            <div style="
                background-color:transparent;  /* ä½ æƒ³è¦çš„é¡è‰² */
                padding:8px 12px;
                border-radius:12px;
                display:inline-block;
            ">
                â³ åˆ†æä¸­...
            </div>
            """,
            unsafe_allow_html=True
        )
    
    # AI åˆ†æ
    context_paragraphs = retrieve_context(user_input)
    context_text = "\n\n".join(context_paragraphs)
    if st.session_state.user_file_text:
        context_text += "\n\n---ä½¿ç”¨è€…ä¸Šå‚³çš„é™„ä»¶å…§å®¹---\n\n" + st.session_state.user_file_text

    answer = get_gemini_response(user_input, context_text, st.session_state.chat_history)

    # 4ï¸âƒ£ å°‡å ä½ spinner æ›¿æ›æˆæœ€çµ‚å›ç­”
    answer_placeholder.empty()  # æ¸…é™¤ spinner
    with chat_container:
        with st.chat_message("assistant"):
            st.markdown(answer)
    # æ›´æ–°æ­·å²å°è©±
    st.session_state.chat_history.append(("ä½¿ç”¨è€…", user_input))
    st.session_state.chat_history.append(("Agent", answer))



  

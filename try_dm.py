import streamlit as st
import os
import time
import fitz
import google.generativeai as genai
import pickle
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.docstore.document import Document


    
from docx import Document as DocxDocument
import pandas as pd
from pptx import Presentation
from PyPDF2 import PdfReader



# --- Gemini è¨­å®š ---
GOOGLE_API_KEY = "AIzaSyDUYOXI8HQPJ4IsxaWzLoilwYwQw6Uq4Dg"
genai.configure(api_key=GOOGLE_API_KEY)
gen_model = genai.GenerativeModel("gemini-2.5-flash")

# --- å‘é‡æ¨¡å‹ ---
embedding_model = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-mpnet-base-v2")
# --- æ–‡ä»¶è®€å–èˆ‡åˆ‡æ®µè½ï¼ˆæ ¸å¿ƒçŸ¥è­˜åº«ï¼‰ ---

# --- å»ºç«‹å‘é‡è³‡æ–™åº« (FAISS) ---
start = time.perf_counter()
vector_store = FAISS.load_local("/workspaces/cc/faiss_path",
                                embedding_model, allow_dangerous_deserialization=True)
faiss_load_time = time.perf_counter() - start
st.write(f"âœ… FAISS è¼‰å…¥è€—æ™‚: {faiss_load_time:.2f} ç§’")


# è¼‰å…¥ documents
start = time.perf_counter()
with open("documents.pkl", "rb") as f:
    documents = pickle.load(f)
docs_load_time = time.perf_counter() - start
st.write(f"âœ… documents è¼‰å…¥è€—æ™‚: {docs_load_time:.2f} ç§’")


# --- RAG æª¢ç´¢ ---
def retrieve_context(question, top_k=3):
    start = time.perf_counter()
    docs = vector_store.similarity_search(question, k=top_k)
    search_time = time.perf_counter() - start
    st.write(f"âœ… æª¢ç´¢è€—æ™‚: {search_time:.2f} ç§’")
    return [d.page_content for d in docs]

# --- Gemini å›ç­”ç”Ÿæˆï¼ˆåŠ å…¥æ­·å²å°è©±ï¼‰ ---
def get_gemini_response(user_question, context_text, history):
    history_text = ""
    for role, msg in history:
        history_text += f"{role}: {msg}\n"

    prompt = f"""
ä½ æ˜¯ä¸€ä½è¡›ç”Ÿç¦åˆ©éƒ¨è³‡è¨Šè™•è™•é•·ï¼Œè² è²¬æä¾›é—œæ–¼é†«ç™‚æ©Ÿæ§‹è³‡è¨Šè¨ˆç•«ã€ç³»çµ±å»ºç½®ã€AI å°å…¥ï¼Œä»¥åŠç›¸é—œæ¡è³¼èˆ‡ç°½æ ¸äº‹é …çš„å°ˆæ¥­æŒ‡å°ã€‚  
ä½ ç†Ÿæ‚‰è³‡å®‰è¦ç¯„ã€å€‹è³‡ä¿è­·æ³•ã€é†«ç™‚è³‡è¨Šç®¡ç†æ³•è¦ï¼Œä»¥åŠæ”¿åºœæ¡è³¼æ³•ç­‰ç›¸é—œå›è¦†ã€‚  

ä½ çš„å›ç­”éœ€åš´è¬¹ã€å®¢è§€ï¼Œæ‡‰ä¾æ“šæ ¸å¿ƒçŸ¥è­˜åº«æä¾›çš„å…§å®¹ã€‚
è‹¥æœ‰ä½¿ç”¨è€…ä¸Šå‚³é™„ä»¶ï¼Œè«‹ä¸€ä½µè§£è®€ä½¿ç”¨è€…ä¸Šå‚³çš„é™„ä»¶å…§å®¹ã€‚
å°æ–¼åŒä¸€è­°é¡Œï¼Œå›ç­”å…§å®¹éœ€ä¿æŒä¸€è‡´ï¼Œé¿å…å‰å¾ŒçŸ›ç›¾ã€‚  
é‡å°ä¸€èˆ¬æ°‘çœ¾æˆ–éå°ˆæ¥­äººå£«ï¼Œè«‹ä»¥æ·ºé¡¯æ˜“æ‡‚çš„æ–¹å¼èªªæ˜å°ˆæ¥­è¡“èªå’Œæµç¨‹ã€‚  
è«‹é¿å…è‡†æ¸¬æˆ–æ¨è«–æœªæ˜è¼‰å…§å®¹ï¼Œå›ç­”æ™‚ä¿æŒæ­£å¼ã€æ¸…æ¥šã€æ˜“æ‡‚çš„èªæ°£ï¼Œç”¨å­—é£è©ä¾æ“šçŸ¥è­˜åº«é¢¨æ ¼ã€‚  
è«‹ä½¿ç”¨ **ç´”æ–‡å­—è¼¸å‡º**ï¼Œä¸è¦åŒ…å« HTML æ¨™ç±¤æˆ–æ ¼å¼åŒ–ç¬¦è™Ÿï¼Œä¾‹å¦‚ <b>ã€<i> ç­‰ã€‚
---å°è©±æ­·å²---
{history_text}

---æ ¸å¿ƒçŸ¥è­˜åº«åŠä½¿ç”¨è€…ä¸Šå‚³çš„é™„ä»¶å…§å®¹---
{context_text}

---ä½¿ç”¨è€…å•é¡Œ---
{user_question}
"""
    start = time.perf_counter()
    response = gen_model.generate_content(prompt)
    gemini_time = time.perf_counter() - start
    st.write(f"âœ… Gemini API è€—æ™‚: {gemini_time:.2f} ç§’")
    return response.text


import streamlit as st
import fitz

# --- åˆå§‹åŒ– ---
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []
if "user_file_text" not in st.session_state:
    st.session_state.user_file_text = ""

st.set_page_config(page_title="è³‡è¨Šè™• AI Agent", layout="wide")

# --- è‡ªè¨‚ CSS ---
st.markdown("""
<style>
.bottom-container {
    position: fixed;
    bottom: 0;
    left: 0;
    right: 0;
    padding: 12px;
    background-color: white;
    border-top: 1px solid #ddd;
    display: flex;
    align-items: center;
    gap: 8px;
}
.input-wrapper {
    flex: 1;
    position: relative;
}
.input-box {
    width: 100%;
    padding: 12px 40px 12px 12px;
    border: 1px solid #ccc;
    border-radius: 20px;
    font-size: 15px;
}
.send-btn {
    position: absolute;
    right: 6px;
    top: 10%;
    transform: translateY(-50%);
    background-color: black;
    color: white;
    border: none;
    border-radius: 50%;
    width: 28px;
    height: 28px;
    font-size: 14px;
    cursor: pointer;
}
.send-btn:hover {
    background-color: #333;
}
</style>
""", unsafe_allow_html=True)

st.title("è³‡è¨Šè™• AI Agent")

# --- èŠå¤©é¡¯ç¤ºå€ ---
chat_container = st.container()
for role, msg in st.session_state.chat_history:
    with chat_container:
        with st.chat_message("user" if role == "ä½¿ç”¨è€…" else "assistant"):
            st.markdown(msg)

# --- åº•éƒ¨è¼¸å…¥æ¡† ---
st.markdown('<div class="bottom-container">', unsafe_allow_html=True)

with st.form("chat_form", clear_on_submit=True):
    cols = st.columns([2, 8, 0.6])

    # ä¸Šå‚³æª”æ¡ˆ
    with cols[0]:
        uploaded_file = st.file_uploader(
            "ğŸ“„ æ–‡ä»¶",
            type=["pdf", "docx","doc","txt","xls","xlsx","ppt","pptx"],
            key="file_uploader",
            label_visibility="collapsed",
            accept_multiple_files=True
        )
        if uploaded_file:
            ext = uploaded_file.name.lower().split(".")[-1]
            file_text = ""
            if ext == "pdf":
                pdf_document = fitz.open(stream=uploaded_file.read(), filetype="pdf")
                for page in pdf_document:
                    file_text += page.get_text() + "\n\n"
            elif ext in ["docx", "doc"]:
                doc = DocxDocument(uploaded_file)
                file_text = "\n".join(p.text for p in doc.paragraphs if p.text.strip())
            elif ext == "txt":
                file_text = uploaded_file.read().decode("utf-8")

            elif ext in ["xls", "xlsx"]:
                df = pd.read_excel(uploaded_file)
                file_text = df.to_csv(index=False)  # è½‰æˆæ–‡å­—
            
            elif ext in ["ppt", "pptx"]:
                prs = Presentation(uploaded_file)
                for slide in prs.slides:
                    for shape in slide.shapes:
                        if hasattr(shape, "text"):
                            file_text += shape.text + "\n"
            
            
            
            st.session_state.user_file_text = file_text
            st.success(f"{uploaded_file.name} å·²æˆåŠŸè®€å–")
  


    # è¼¸å…¥æ¡† + é€å‡ºæŒ‰éˆ•
    with cols[1]:
        user_input = st.text_area("è«‹è¼¸å…¥æ‚¨çš„å•é¡Œâ€¦", key="chat_input", label_visibility="collapsed",height=50)
        
    with cols[2]:
        submit = st.form_submit_button("â¤", use_container_width=False)


st.markdown('</div>', unsafe_allow_html=True)

# --- è™•ç†è¨Šæ¯ ---
if submit and user_input:
    # 1ï¸âƒ£ é¡¯ç¤ºä½¿ç”¨è€…è¨Šæ¯
    with chat_container:
        with st.chat_message("user"):
            st.markdown(user_input)

    # 2ï¸âƒ£ åœ¨èŠå¤©å€å»ºç«‹ AI å›ç­”çš„å ä½å®¹å™¨
    with chat_container:
        answer_placeholder = st.empty()  # å ä½
        with answer_placeholder.container():
            # åœ¨å ä½å®¹å™¨è£¡å…ˆé¡¯ç¤º spinner
            with st.chat_message("assistant"):
                st.markdown(
            """
            <div style="
                background-color:transparent;  /* ä½ æƒ³è¦çš„é¡è‰² */
                padding:8px 12px;
                border-radius:12px;
                display:inline-block;
            ">
                â³ åˆ†æä¸­...
            </div>
            """,
            unsafe_allow_html=True
        )
    
    # AI åˆ†æ
    context_paragraphs = retrieve_context(user_input)
    context_text = "\n\n".join(context_paragraphs)
    if st.session_state.user_file_text:
        context_text += "\n\n---ä½¿ç”¨è€…ä¸Šå‚³çš„é™„ä»¶å…§å®¹---\n\n" + st.session_state.user_file_text

    answer = get_gemini_response(user_input, context_text, st.session_state.chat_history)

    # 4ï¸âƒ£ å°‡å ä½ spinner æ›¿æ›æˆæœ€çµ‚å›ç­”
    answer_placeholder.empty()  # æ¸…é™¤ spinner
    with chat_container:
        with st.chat_message("assistant"):
            st.markdown(answer)
    # æ›´æ–°æ­·å²å°è©±
    st.session_state.chat_history.append(("ä½¿ç”¨è€…", user_input))
    st.session_state.chat_history.append(("Agent", answer))



  
